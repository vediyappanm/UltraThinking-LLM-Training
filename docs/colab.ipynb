{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULTRATHINK Model Training - Google Colab\n",
    "\n",
    "This notebook provides a complete training pipeline for the ULTRATHINK model on Google Colab.\n",
    "\n",
    "## Features\n",
    "- ‚úÖ Mixture of Experts (MoE)\n",
    "- ‚úÖ Dynamic Reasoning Engine (DRE)\n",
    "- ‚úÖ Constitutional AI\n",
    "- ‚úÖ RLHF (optional)\n",
    "- ‚úÖ Multimodal capabilities (optional)\n",
    "- ‚úÖ MLflow tracking\n",
    "- ‚úÖ Google Drive integration for checkpoints\n",
    "\n",
    "## Requirements\n",
    "- GPU Runtime (T4/V100/A100)\n",
    "- High RAM option recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for saving checkpoints)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory\n",
    "!mkdir -p /content/drive/MyDrive/ULTRATHINK_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/vediyappanm/UltraThinking-LLM-Training.git\n",
    "%cd UltraThinking-LLM-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Install flash attention (if supported)\n",
    "try:\n",
    "    !pip install -q flash-attn --no-build-isolation\n",
    "    print(\"‚úÖ Flash Attention installed\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Flash Attention not available (will use standard attention)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure MLflow API Key (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Option 1: Local MLflow (default)\n",
    "os.environ['MLFLOW_TRACKING_URI'] = 'file:./mlruns'\n",
    "\n",
    "# Option 2: Remote MLflow server (uncomment and configure)\n",
    "# os.environ['MLFLOW_TRACKING_URI'] = 'https://your-mlflow-server.com'\n",
    "# os.environ['MLFLOW_TRACKING_USERNAME'] = 'your-username'\n",
    "# os.environ['MLFLOW_TRACKING_PASSWORD'] = 'your-password'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Configurations\n",
    "\n",
    "Choose your training configuration based on available GPU resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 1: Small Model (T4 GPU, 16GB RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test training\n",
    "!python train_advanced.py \\\n",
    "  --config configs/train_small.yaml \\\n",
    "  --override \\\n",
    "    training.batch_size=2 \\\n",
    "    training.gradient_accumulation_steps=16 \\\n",
    "    model.max_seq_length=512 \\\n",
    "    data.train_samples=10000 \\\n",
    "    data.val_samples=1000 \\\n",
    "    training.num_epochs=1 \\\n",
    "    logging.use_mlflow=true \\\n",
    "    output.output_dir=/content/drive/MyDrive/ULTRATHINK_checkpoints/small_model \\\n",
    "  --run-name \"colab_small_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 2: Medium Model (V100/A100 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medium model with advanced features\n",
    "!python train_advanced.py \\\n",
    "  --config configs/train_medium.yaml \\\n",
    "  --override \\\n",
    "    training.batch_size=4 \\\n",
    "    training.gradient_accumulation_steps=8 \\\n",
    "    advanced.enable_moe=true \\\n",
    "    advanced.enable_dre=true \\\n",
    "    advanced.enable_constitutional=true \\\n",
    "    data.streaming=false \\\n",
    "    data.train_samples=50000 \\\n",
    "    data.val_samples=5000 \\\n",
    "    training.num_epochs=2 \\\n",
    "    logging.use_mlflow=true \\\n",
    "    output.output_dir=/content/drive/MyDrive/ULTRATHINK_checkpoints/medium_model \\\n",
    "  --run-name \"colab_medium_training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 3: Large Model (A100 40/80GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-scale training with all features\n",
    "!python train_advanced.py \\\n",
    "  --config configs/train_large.yaml \\\n",
    "  --override \\\n",
    "    training.batch_size=8 \\\n",
    "    advanced.enable_moe=true \\\n",
    "    advanced.enable_dre=true \\\n",
    "    advanced.enable_constitutional=true \\\n",
    "    advanced.enable_multimodal=true \\\n",
    "    data.streaming=true \\\n",
    "    logging.use_mlflow=true \\\n",
    "    output.output_dir=/content/drive/MyDrive/ULTRATHINK_checkpoints/large_model \\\n",
    "  --run-name \"colab_large_training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Training with Command-Line Interface\n",
    "\n",
    "Alternatively, use the original training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training with specific parameters\n",
    "!python train_ultrathink.py \\\n",
    "  --use_mlflow \\\n",
    "  --mlflow_experiment \"ULTRATHINK-Colab\" \\\n",
    "  --run_name \"custom_colab_run\" \\\n",
    "  --dataset wikitext \\\n",
    "  --hidden_size 1024 \\\n",
    "  --num_layers 12 \\\n",
    "  --num_heads 16 \\\n",
    "  --num_kv_heads 8 \\\n",
    "  --batch_size 4 \\\n",
    "  --gradient_accumulation_steps 8 \\\n",
    "  --max_seq_length 2048 \\\n",
    "  --learning_rate 1e-4 \\\n",
    "  --num_epochs 2 \\\n",
    "  --enable_moe \\\n",
    "  --enable_dre \\\n",
    "  --enable_constitutional \\\n",
    "  --use_flash_attention \\\n",
    "  --gradient_checkpointing \\\n",
    "  --use_amp \\\n",
    "  --eval_frequency 1 \\\n",
    "  --output_dir /content/drive/MyDrive/ULTRATHINK_checkpoints/custom_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Monitor Training\n",
    "\n",
    "### View Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View training logs\n",
    "!tail -n 50 ./outputs/*/training.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow UI (Local)\n",
    "\n",
    "To view MLflow metrics in Colab, use ngrok tunnel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ngrok\n",
    "!pip install -q pyngrok\n",
    "\n",
    "# Start MLflow UI in background\n",
    "import subprocess\n",
    "import time\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Start MLflow server\n",
    "mlflow_proc = subprocess.Popen([\"mlflow\", \"ui\", \"--port\", \"5000\"])\n",
    "time.sleep(3)\n",
    "\n",
    "# Create ngrok tunnel\n",
    "public_url = ngrok.connect(5000)\n",
    "print(f\"\\nüöÄ MLflow UI available at: {public_url}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resume Training from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume from saved checkpoint\n",
    "!python train_advanced.py \\\n",
    "  --config configs/train_medium.yaml \\\n",
    "  --resume /content/drive/MyDrive/ULTRATHINK_checkpoints/medium_model/checkpoint_epoch_0.pt \\\n",
    "  --run-name \"resumed_training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fine-tuning with RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune with RLHF\n",
    "!python train_advanced.py \\\n",
    "  --config configs/train_medium.yaml \\\n",
    "  --init-from /content/drive/MyDrive/ULTRATHINK_checkpoints/medium_model/final_model \\\n",
    "  --override \\\n",
    "    advanced.enable_rlhf=true \\\n",
    "    training.learning_rate=1e-5 \\\n",
    "    training.num_epochs=1 \\\n",
    "    rlhf.rlhf_frequency=1 \\\n",
    "    output.output_dir=/content/drive/MyDrive/ULTRATHINK_checkpoints/rlhf_model \\\n",
    "  --run-name \"rlhf_finetuning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained model\n",
    "import json\n",
    "\n",
    "# Load evaluation results\n",
    "with open('./outputs/*/evaluation_results.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(\"\\nüìä Evaluation Results:\")\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model in HuggingFace format\n",
    "import torch\n",
    "from src.models.ultrathink import UltraThinkModel\n",
    "\n",
    "# Load trained model\n",
    "model_path = \"/content/drive/MyDrive/ULTRATHINK_checkpoints/medium_model/final_model\"\n",
    "model = UltraThinkModel.from_pretrained(model_path)\n",
    "\n",
    "# Save for deployment\n",
    "export_path = \"/content/drive/MyDrive/ULTRATHINK_checkpoints/exported_model\"\n",
    "model.save_pretrained(export_path)\n",
    "\n",
    "print(f\"‚úÖ Model exported to: {export_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quick Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference test\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = UltraThinkModel.from_pretrained(export_path)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Test generation\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=100,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nü§ñ Generated Text:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download results\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Create archive\n",
    "shutil.make_archive('training_results', 'zip', './outputs')\n",
    "\n",
    "# Download\n",
    "files.download('training_results.zip')\n",
    "print(\"‚úÖ Results downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips & Troubleshooting\n",
    "\n",
    "### Out of Memory (OOM) Errors\n",
    "- Reduce `batch_size`\n",
    "- Increase `gradient_accumulation_steps`\n",
    "- Reduce `max_seq_length`\n",
    "- Enable `gradient_checkpointing`\n",
    "- Disable advanced features temporarily\n",
    "\n",
    "### Slow Training\n",
    "- Enable `use_flash_attention` if supported\n",
    "- Use larger `batch_size` if memory allows\n",
    "- Enable `use_amp` for mixed precision\n",
    "- Reduce `num_workers` if CPU bottleneck\n",
    "\n",
    "### Connection Issues\n",
    "- Save checkpoints frequently to Google Drive\n",
    "- Use `--continuous` flag for long training\n",
    "- Keep browser tab active to prevent disconnection\n",
    "\n",
    "## Resources\n",
    "- üìñ [Documentation](https://github.com/vediyappanm/UltraThinking-LLM-Training)\n",
    "- üí¨ [Issues](https://github.com/vediyappanm/UltraThinking-LLM-Training/issues)\n",
    "- üìä [Model Card](MODEL_CARD.md)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
