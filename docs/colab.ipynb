{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ULTRATHINK: Colab Quickstart\n\nRun a quick sanity check and a small real-data training in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "env-check"
      },
      "source": [
        "import torch, platform\n",
        "!nvidia-smi || echo 'No NVIDIA GPU available'\n",
        "print('Python:', platform.python_version())\n",
        "print('CUDA available:', torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clone-install"
      },
      "source": [
        "!git clone https://github.com/vediyappanm/UltraThinking-LLM-Training.git\n",
        "%cd UltraThinking-LLM-Training/deep\n",
        "!pip install --upgrade pip\n",
        "# Install PyTorch (CUDA wheels for Colab)\n",
        "!pip install torch --index-url https://download.pytorch.org/whl/cu121\n",
        "# Core deps\n",
        "!pip install \"transformers>=4.41.0\" datasets einops tqdm wandb accelerate\n",
        "# Optional: FlashAttention (may fail on some Colab GPUs)\n",
        "# !pip install flash-attn --no-build-isolation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "env-vars"
      },
      "source": [
        "import os\n",
        "os.environ['TORCHDYNAMO_DISABLE'] = '1'\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "print('Env set.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Sanity run on dummy data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sanity-run"
      },
      "source": [
        "!python train_ultrathink.py \\\n  --dataset dummy --train_samples 2000 --val_samples 200 \\\n  --vocab_size 50257 --hidden_size 384 --num_layers 4 --num_heads 6 --num_kv_heads 6 \\\n  --intermediate_size 1536 --max_seq_length 256 \\\n  --batch_size 4 --gradient_accumulation_steps 8 \\\n  --learning_rate 5e-4 --use_amp --gradient_checkpointing \\\n  --num_epochs 1 \\\n  --output_dir ./outputs/sanity_dummy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Small real-data run (C4 streaming)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4-run"
      },
      "source": [
        "!python train_ultrathink.py \\\n  --dataset c4 --dataset_subset en --streaming \\\n  --enable_dre --dre_warmup_steps 500 \\\n  --amp_warmup_steps 200 \\\n  --tokenizer_name gpt2 \\\n  --vocab_size 50257 \\\n  --hidden_size 384 --num_layers 4 --num_heads 6 --num_kv_heads 6 \\\n  --intermediate_size 1536 --max_seq_length 512 \\\n  --batch_size 1 --gradient_accumulation_steps 64 \\\n  --learning_rate 5e-5 --weight_decay 0.1 \\\n  --warmup_steps 2000 \\\n  --use_amp --gradient_checkpointing \\\n  --eval_frequency 1 \\\n  --output_dir ./outputs/ultrathink_c4_seq512_sdpa_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Switch datasets (examples)\n\n- See `docs/datasets.md` for more.\n- Add `--use_wandb --run_name ultrathink_colab` to log to W&B."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
